{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc52ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/pgm')\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pgm.train_cf import cf_epoch\n",
    "from pgm.train_pgm import setup_dataloaders\n",
    "from pgm.flow_pgm import ChestPGM\n",
    "\n",
    "class Hparams:\n",
    "    def update(self, dict):\n",
    "        for k, v in dict.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a7bf1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "predictor_path = '/workspace/checkpoints/a_r_s_f/aux_mimic_60k-auxg/checkpoint.pt'\n",
    "print(f'\\nLoading predictor checkpoint: {predictor_path}')\n",
    "predictor_checkpoint = torch.load(predictor_path)\n",
    "predictor_args = Hparams()\n",
    "predictor_args.update(predictor_checkpoint['hparams'])\n",
    "assert predictor_args.dataset == 'morphomnist'\n",
    "predictor = MorphoMNISTPGM(predictor_args).cuda()\n",
    "predictor.load_state_dict(predictor_checkpoint['ema_model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ab73d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pgm_path = '/workspace/checkpoints/a_r_s_f/pgm_60k-pgmg/checkpoint.pt'\n",
    "print(f'\\nLoading PGM checkpoint: {pgm_path}')\n",
    "pgm_checkpoint = torch.load(pgm_path)\n",
    "pgm_args = Hparams()\n",
    "pgm_args.update(pgm_checkpoint['hparams'])\n",
    "assert pgm_args.dataset == 'morphomnist'\n",
    "pgm = MorphoMNISTPGM(pgm_args).cuda()\n",
    "pgm.load_state_dict(pgm_checkpoint['ema_model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be20abe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_fm(fm_path):\n",
    "    print(f'\\nLoading flow matching checkpoint: {fm_path}')\n",
    "    fm_checkpoint = torch.load(fm_path)\n",
    "    fm_args = Hparams()\n",
    "    fm_args.update(fm_checkpoint['hparams'])\n",
    "    fm_args.data_dir = '../datasets/'\n",
    "\n",
    "    # init model\n",
    "    assert fm_args.hps == 'morphomnist'\n",
    "    from flow_model import DeeperUnet\n",
    "    fm=DeeperUnet(fm_args).cuda()\n",
    "    fm.load_state_dict(fm_checkpoint['ema_model_state_dict'])\n",
    "    return fm, fm_args\n",
    "\n",
    "model_name = ''\n",
    "fm_path = '../checkpoints/t_i_d/'+model_name+'/checkpoint.pt'\n",
    "fm, fm_args = load_fm(fm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dfd3b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from morphomnist.morpho import ImageMorphology\n",
    "# Refer to https://github.com/dccastro/Morpho-MNIST for details on Morpho-MNIST\n",
    "\n",
    "def get_intensity(x, threshold=0.5):\n",
    "    x = x.detach().cpu().numpy()[:, 0]\n",
    "    x_min, x_max = x.min(axis=(1, 2), keepdims=True), x.max(axis=(1, 2), keepdims=True)\n",
    "    mask = (x >= x_min + (x_max - x_min) * threshold)\n",
    "    return np.array([np.median(i[m]) for i, m in zip(x, mask)])\n",
    "\n",
    "def img_thickness(img, threshold, scale):\n",
    "    return ImageMorphology(np.asarray(img), threshold, scale).mean_thickness\n",
    "\n",
    "def unpack(args):\n",
    "    return img_thickness(*args)\n",
    "\n",
    "def get_thickness(x, threshold=0.5, scale=4, pool=None, chunksize=100):\n",
    "    imgs = x.detach().cpu().numpy()[:, 0]\n",
    "    args = ((img, threshold, scale) for img in imgs)\n",
    "    if pool is None:\n",
    "        gen = map(unpack, args)\n",
    "    else:\n",
    "        gen = pool.imap(unpack, args, chunksize=chunksize)\n",
    "    results = tqdm(gen, total=len(imgs), unit='img', ascii=True)\n",
    "    return list(results)\n",
    "\n",
    "def fm_preprocess(pa: Dict[str, Tensor], input_res: int = 28) -> Tensor:\n",
    "    # concatenate parents and expand to input resolution for vae input\n",
    "    for k,v in pa.items():\n",
    "        pa[k]=pa[k].unsqueeze(-1)\n",
    "    return pa\n",
    "\n",
    "    \n",
    "@torch.no_grad()\n",
    "def cf_epoch(\n",
    "    vae: nn.Module, \n",
    "    pgm: nn.Module, \n",
    "    predictor: nn.Module, \n",
    "    dataloaders: Dict[str, DataLoader],\n",
    "    do_pa: Optional[str] = None, \n",
    "    te_cf: bool = False\n",
    ") -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    vae.eval()\n",
    "    pgm.eval()\n",
    "    predictor.eval()\n",
    "    dag_vars = list(pgm.variables.keys())\n",
    "    preds = {k: [] for k in dag_vars}\n",
    "    targets = {k: [] for k in dag_vars}\n",
    "    x_counterfactuals = []\n",
    "    train_set = copy.deepcopy(dataloaders['train'].dataset.samples)\n",
    "    loader = tqdm(enumerate(dataloaders['test']), total=len(\n",
    "        dataloaders['test']), mininterval=0.1)\n",
    "\n",
    "    for _, batch in loader:\n",
    "        bs = batch['x'].shape[0]\n",
    "        batch = preprocess(batch)\n",
    "        pa = {k: v for k, v in batch.items() if k != 'x'}\n",
    "        # randomly intervene on a single parent do(pa_k), pa_k ~ p(pa_k)\n",
    "        do = {}\n",
    "        if do_pa is not None:\n",
    "            idx = torch.randperm(train_set[do_pa].shape[0])\n",
    "            do[do_pa] = train_set[do_pa].clone()[idx][:bs]\n",
    "        else: # random interventions\n",
    "            while not do:\n",
    "                for k in dag_vars:\n",
    "                    if torch.rand(1) > 0.5:  # coin flip to intervene on pa_k\n",
    "                        idx = torch.randperm(train_set[k].shape[0])\n",
    "                        do[k] = train_set[k].clone()[idx][:bs]\n",
    "        do = preprocess(do)\n",
    "        # infer counterfactual parents\n",
    "        cf_pa = pgm.counterfactual(obs=pa, intervention=do, num_particles=1)\n",
    "        _pa = vae_preprocess({k: v.clone() for k, v in pa.items()})\n",
    "        _cf_pa = vae_preprocess({k: v.clone() for k, v in cf_pa.items()})\n",
    "        # abduct exogenous noise z\n",
    "        t_z = t_u = 0.1  # sampling temp\n",
    "        z = vae.abduct(batch['x'], parents=_pa, t=t_z)\n",
    "        if vae.cond_prior:\n",
    "            z = [z[i]['z'] for i in range(len(z))]\n",
    "        # forward vae with observed parents\n",
    "        rec_loc, rec_scale = vae.forward_latents(z, parents=_pa)\n",
    "        # abduct exogenous noise u\n",
    "        u = (batch['x'] - rec_loc) / rec_scale.clamp(min=1e-12)\n",
    "        if vae.cond_prior and te_cf:  # g(z*, pa*)\n",
    "            # infer counterfactual mediator z*\n",
    "            cf_z = vae.abduct(x=batch['x'], parents=_pa, cf_parents=_cf_pa, alpha=0.65)\n",
    "            cf_loc, cf_scale = vae.forward_latents(cf_z, parents=_cf_pa)\n",
    "        else:  # g(z, pa*)\n",
    "            cf_loc, cf_scale = vae.forward_latents(z, parents=_cf_pa)\n",
    "        cf_scale = cf_scale * t_u\n",
    "        cfs = {'x':  torch.clamp(cf_loc + cf_scale * u, min=-1, max=1)}\n",
    "        cfs.update(cf_pa)\n",
    "        x_counterfactuals.extend(cfs['x'])\n",
    "        # predict labels of inferred counterfactuals\n",
    "        preds_cf = predictor.predict(**cfs)\n",
    "        for k, v in preds_cf.items():\n",
    "            preds[k].extend(v)\n",
    "        # targets are the interventions and/or counterfactual parents\n",
    "        for k in targets.keys():\n",
    "            t_k = do[k].clone() if k in do.keys() else cfs[k].clone()\n",
    "            targets[k].extend(t_k)\n",
    "    for k, v in targets.items():\n",
    "        targets[k] = torch.stack(v).squeeze().cpu()\n",
    "        preds[k] = torch.stack(preds[k]).squeeze().cpu()\n",
    "    x_counterfactuals = torch.stack(x_counterfactuals).cpu()\n",
    "    return targets, preds, x_counterfactuals\n",
    "\n",
    "\n",
    "def eval_cf_loop(\n",
    "    vae: nn.Module,\n",
    "    pgm: nn.Module,\n",
    "    predictor: nn.Module,\n",
    "    dataloaders: Dict[str, DataLoader],\n",
    "    file: IO[str],\n",
    "    total_effect: bool = False,\n",
    "    seeds: List[int] = [0, 1, 2],\n",
    "):\n",
    "    for do_pa in ['thickness', 'intensity', 'digit', None]:  # \"None\" is for random interventions\n",
    "        acc_runs = []\n",
    "        mae_runs = {\n",
    "            'thickness': {'predicted': [], 'measured': []},\n",
    "            'intensity': {'predicted': [], 'measured': []}\n",
    "        }\n",
    "\n",
    "        for seed in seeds:\n",
    "            print(f'do({(do_pa if do_pa is not None else \"random\")}), seed {seed}:')\n",
    "            assert vae.cond_prior if total_effect else True\n",
    "            targets, preds, x_cfs = cf_epoch(vae, pgm, predictor, dataloaders, do_pa, total_effect)\n",
    "            acc = (targets['digit'].argmax(-1).numpy() == preds['digit'].argmax(-1).numpy()).mean()\n",
    "            print(f'predicted digit acc:', acc)\n",
    "            # evaluate inferred cfs using true causal mechanisms\n",
    "            measured = {}\n",
    "            measured['intensity'] = torch.tensor(get_intensity((x_cfs + 1.0) * 127.5))\n",
    "            with multiprocessing.Pool() as pool:\n",
    "                measured['thickness'] = torch.tensor(get_thickness((x_cfs + 1.0) * 127.5, pool=pool, chunksize=250))\n",
    "\n",
    "            mae = {'thickness': {}, 'intensity': {}}\n",
    "            for k in ['thickness', 'intensity']:\n",
    "                min_max = dataloaders['train'].dataset.min_max[k]\n",
    "                _min, _max = min_max[0], min_max[1]\n",
    "                preds_k = ((preds[k] + 1) / 2) * (_max - _min) + _min\n",
    "                targets_k = ((targets[k] + 1) / 2) * (_max - _min) + _min\n",
    "                mae[k]['predicted'] = (targets_k - preds_k).abs().mean().item()\n",
    "                mae[k]['measured'] = (targets_k - measured[k]).abs().mean().item()\n",
    "                print(f'predicted {k} mae:', mae[k]['predicted'])\n",
    "                print(f'measured {k} mae:', mae[k]['measured'])\n",
    "\n",
    "            acc_runs.append(acc)\n",
    "            for k in ['thickness', 'intensity']:\n",
    "                mae_runs[k]['predicted'].append(mae[k]['predicted'])\n",
    "                mae_runs[k]['measured'].append(mae[k]['measured'])\n",
    "\n",
    "            file.write(\n",
    "                f'\\ndo({(do_pa if do_pa is not None else \"random\")}) | digit acc: {acc}, ' +\n",
    "                f'thickness mae (predicted): {mae[\"thickness\"][\"predicted\"]}, ' +\n",
    "                f'thickness mae (measured): {mae[\"thickness\"][\"measured\"]}, ' +\n",
    "                f'intensity mae (predicted): {mae[\"intensity\"][\"predicted\"]}, ' +\n",
    "                f'intensity mae (measured): {mae[\"intensity\"][\"measured\"]} | seed {seed}'\n",
    "            )\n",
    "            file.flush()\n",
    "            gc.collect()\n",
    "\n",
    "        v = 'Total effect: '+ str(total_effect)\n",
    "        file.write(\n",
    "            f'\\n{(v if vae.cond_prior else \"\")}\\n' +\n",
    "            f'digit acc | mean: {np.array(acc_runs).mean()} - std: {np.array(acc_runs).std()}\\n' +\n",
    "            f'thickness mae (predicted) | mean: {np.array(mae_runs[\"thickness\"][\"predicted\"]).mean()} - std: {np.array(mae_runs[\"thickness\"][\"predicted\"]).std()}\\n' +\n",
    "            f'thickness mae (measured) | mean: {np.array(mae_runs[\"thickness\"][\"measured\"]).mean()} - std: {np.array(mae_runs[\"thickness\"][\"measured\"]).std()}\\n' +\n",
    "            f'intensity mae (predicted) | mean: {np.array(mae_runs[\"intensity\"][\"predicted\"]).mean()} - std: {np.array(mae_runs[\"intensity\"][\"predicted\"]).std()}\\n' +\n",
    "            f'intensity mae (measured) | mean: {np.array(mae_runs[\"intensity\"][\"measured\"]).mean()} - std: {np.array(mae_runs[\"intensity\"][\"measured\"]).std()}\\n'\n",
    "        )\n",
    "        file.flush()\n",
    "    return\n",
    "\n",
    "for model_name in [\n",
    "'add your model name(s) here'\n",
    "]:\n",
    "    file = open(f'./eval_{model_name}.txt', 'a')\n",
    "    vae_path = '../checkpoints/'+model_name+'/checkpoint.pt'\n",
    "    vae, vae_args = load_vae(vae_path)\n",
    "    assert pgm_args.dataset == 'morphomnist'\n",
    "    pgm_args.data_dir = 'your dataset dir here'\n",
    "    pgm_args.bs = 32\n",
    "    dataloaders = setup_dataloaders(pgm_args)\n",
    "    eval_cf_loop(vae, pgm, predictor, dataloaders, file)\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
